[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling with tidymodels in R",
    "section": "",
    "text": "Preface\nThis material is from the DataCamp course Modeling with tidymodels in R by David Svancer.\nCourse Description: Tidymodels (Kuhn and Wickham 2024) is a powerful suite of R packages designed to streamline machine learning workflows. Learn to split datasets for cross-validation, preprocess data with tidymodels’ recipe package, and fine-tune machine learning algorithms. You’ll learn key concepts such as defining model objects and creating modeling workflows. Then, you’ll apply your skills to predict home prices and classify employees by their risk of leaving a company.\nReminder to self: each *.qmd file contains one and only one chapter, and a chapter is defined by the first-level heading #.\n\n\n\n\nKuhn, Max, and Hadley Wickham. 2024. Tidymodels: Easily Install and Load the Tidymodels Packages. https://tidymodels.tidymodels.org."
  },
  {
    "objectID": "01-MWTM.html#the-tidymodels-ecosystem-video",
    "href": "01-MWTM.html#the-tidymodels-ecosystem-video",
    "title": "1  Machine Learning with tidymodels",
    "section": "The tidymodels ecosystem video",
    "text": "The tidymodels ecosystem video"
  },
  {
    "objectID": "01-MWTM.html#tidymodels-packages",
    "href": "01-MWTM.html#tidymodels-packages",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.1 Tidymodels packages",
    "text": "1.1 Tidymodels packages\ntidymodels is a collection of machine learning packages designed to simplify the machine learning workflow in R.\nIn this exercise, you will assign each package within the tidymodels ecosystem to its corresponding process within the machine learning workflow.\n\n\n\n\n\n\n\n\n\nThe core packages within tidymodels are designed to help with every stage in a machine learning workflow."
  },
  {
    "objectID": "01-MWTM.html#creating-training-and-test-datasets",
    "href": "01-MWTM.html#creating-training-and-test-datasets",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.2 Creating training and test datasets",
    "text": "1.2 Creating training and test datasets\nThe rsample package (Frick et al. 2024) is designed to create training and test datasets. Creating a test dataset is important for estimating how a trained model will likely perform on new data. It also guards against overfitting, where a model memorizes patterns that exist only in the training data and performs poorly on new data.\nIn this exercise, you will create training and test datasets from the home_sales data. This data contains information on homes sold in the Seattle, Washington area between 2015 and 2016.\nThe outcome variable in this data is selling_price.\nThe tidymodels package will be pre-loaded in every exercise in the course. The home_sales tibble has also been loaded for you.\n\nlibrary(tidymodels)\nhome_sales &lt;- readRDS(\"./data/home_sales.rds\")\nhead(home_sales)\n\n# A tibble: 6 × 8\n  selling_price home_age bedrooms bathrooms sqft_living sqft_lot sqft_basement\n          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1        487000       10        4      2.5         2540     5001             0\n2        465000       10        3      2.25        1530     1245           480\n3        411000       18        2      2           1130     1148           330\n4        635000        4        3      2.5         3350     4007           800\n5        380000       24        5      2.5         2130     8428             0\n6        495000       21        3      3.5         1650     1577           550\n# ℹ 1 more variable: floors &lt;dbl&gt;\n\n\n\nInstructions\n\nCreate an rsample object, home_split, that contains the instructions for randomly splitting the home_sales data into a training and test dataset. Allocate 70% of the data into training and stratify the results by selling_price.\n\n\nset.seed(156)\n# Create a data split object\nhome_split &lt;- initial_split(home_sales, \n                            prop = 0.70, \n                            strata = selling_price)\nhome_split\n\n&lt;Training/Testing/Total&gt;\n&lt;1042/450/1492&gt;\n\n\n\nCreate a training dataset from home_split called home_training.\n\n\n# Create the training data\nhome_training &lt;- home_split |&gt;\n training()\nstr(home_training)\n\ntibble [1,042 × 8] (S3: tbl_df/tbl/data.frame)\n $ selling_price: num [1:1042] 380000 355000 356000 381000 398000 ...\n $ home_age     : num [1:1042] 24 19 24 25 14 9 37 30 26 7 ...\n $ bedrooms     : num [1:1042] 5 3 2 3 3 3 3 3 3 3 ...\n $ bathrooms    : num [1:1042] 2.5 2.25 1 2 1.5 2.5 2.25 2.25 2.5 2.25 ...\n $ sqft_living  : num [1:1042] 2130 1430 1430 1680 1310 1600 1410 1410 1600 1410 ...\n $ sqft_lot     : num [1:1042] 8428 4777 365904 8946 2996 ...\n $ sqft_basement: num [1:1042] 0 0 420 740 0 0 120 120 0 120 ...\n $ floors       : num [1:1042] 2 2 1 1 2 2 2 2 2 2 ...\n\n\n\nCreate the home_test tibble by passing home_split into the appropriate function for generating test datasets.\n\n\n# Create the test data\nhome_test &lt;- home_split |&gt; \n  testing()\nstr(home_test)\n\ntibble [450 × 8] (S3: tbl_df/tbl/data.frame)\n $ selling_price: num [1:450] 411000 425000 535000 559900 552321 ...\n $ home_age     : num [1:450] 18 11 3 20 29 6 22 25 26 24 ...\n $ bedrooms     : num [1:450] 2 4 4 3 3 3 3 3 5 3 ...\n $ bathrooms    : num [1:450] 2 2.5 2.75 2.75 2.5 2.25 2.5 2.5 3.75 2.25 ...\n $ sqft_living  : num [1:450] 1130 1920 2360 2930 1960 ...\n $ sqft_lot     : num [1:450] 1148 9000 15100 5569 8469 ...\n $ sqft_basement: num [1:450] 330 0 0 1070 0 350 0 0 0 145 ...\n $ floors       : num [1:450] 2 2 1 1 2 2 2 2 2 2 ...\n\n\n\nCheck the number of rows in the training and test datasets by passing them into the nrow() function.\n\n\n# Check number of rows in each dataset\nnrow(home_training)\n\n[1] 1042\n\nnrow(home_test)\n\n[1] 450\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince the home_sales data has 1492 rows, it is appropriate to allocate more rows into the test set. This will provide more data for the model evaluation step."
  },
  {
    "objectID": "01-MWTM.html#distribution-of-outcome-variable-values",
    "href": "01-MWTM.html#distribution-of-outcome-variable-values",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.3 Distribution of outcome variable values",
    "text": "1.3 Distribution of outcome variable values\nStratifying by the outcome variable when generating training and test datasets ensures that the outcome variable values have a similar range in both datasets.\nSince the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.\nIn this exercise, you will calculate summary statistics for the selling_price variable in the training and test datasets. The home_training and home_test tibbles have been loaded from the previous exercise.\n\nInstructions\n\nCalculate the minimum, maximum, mean, and standard deviation of the selling_price variable in home_training.\n\n\n# Distribution of selling_price in training data\nhome_training |&gt;  \n  summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) |&gt; \n  kable()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n478448.6\n80394.43\n\n\n\n\n\n\nCalculate the minimum, maximum, mean, and standard deviation of the selling_price variable in home_test.\n\n\n# Distribution of selling_price in test data\nhome_test |&gt;  \n  summarize(min_sell_price = min(selling_price),\n            max_sell_price = max(selling_price),\n            mean_sell_price = mean(selling_price),\n            sd_sell_price = sd(selling_price)) |&gt; \n  kable()\n\n\n\n\nmin_sell_price\nmax_sell_price\nmean_sell_price\nsd_sell_price\n\n\n\n\n350000\n650000\n480556.9\n82387.91\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe minimum and maximum selling prices in both datasets are the same. The mean and standard deviation are also similar. Stratifying by the outcome variable ensures the model fitting process is performed on a representative sample of the original data."
  },
  {
    "objectID": "01-MWTM.html#linear-regression-with-tidymodels-video",
    "href": "01-MWTM.html#linear-regression-with-tidymodels-video",
    "title": "1  Machine Learning with tidymodels",
    "section": "Linear regression with tidymodels video",
    "text": "Linear regression with tidymodels video"
  },
  {
    "objectID": "01-MWTM.html#fitting-a-linear-regression-model",
    "href": "01-MWTM.html#fitting-a-linear-regression-model",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.4 Fitting a linear regression model",
    "text": "1.4 Fitting a linear regression model\nThe parsnip package (Kuhn and Vaughan 2025) provides a unified syntax for the model fitting process in R.\nWith parsnip, it is easy to define models using the various packages, or engines, that exist in the R ecosystem.\nIn this exercise, you will define a parsnip linear regression object and train your model to predict selling_price using home_age and sqft_living as predictor variables from the home_sales data.\nThe home_training and home_test tibbles that you created in the previous lesson have been loaded into this session.\n\nInstructions\n\nInitialize a linear regression object, linear_model (this is often called a specification and will frequently be stored as linear_spec versus linear_model), with the appropriate parsnip function. Use the \"lm\" engine. Set the mode to \"regression\".\n\n\n# Initialize a linear regression object, linear_model\nlinear_model &lt;- linear_reg() |&gt; \n  # Set the model engine\n  set_engine('lm') |&gt; \n  # Set the model mode\n  set_mode('regression')\nlinear_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nTrain your model to predict selling_price using home_age and sqft_living as predictor variables from the home_training dataset. Print lm_fit to view the model information.\n\n\n# Train the model with the training data\nlm_fit &lt;- linear_model |&gt; \n  fit(selling_price ~ home_age + sqft_living,\n      data = home_training)\n\n# Print lm_fit to view model information\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = selling_price ~ home_age + sqft_living, data = data)\n\nCoefficients:\n(Intercept)     home_age  sqft_living  \n   291587.2      -1550.0        103.8  \n\ntidy(lm_fit) |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n291587.211\n7445.953323\n39.160494\n0\n\n\nhome_age\n-1549.992\n173.052920\n-8.956754\n0\n\n\nsqft_living\n103.789\n2.706057\n38.354335\n0\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou have defined your model with linear_reg() and trained it to predict selling_price using home_age and sqft_living. Printing a parsnip model fit object displays useful model information, such as the training time, model formula used during training, and the estimated model parameters."
  },
  {
    "objectID": "01-MWTM.html#exploring-estimated-model-parameters",
    "href": "01-MWTM.html#exploring-estimated-model-parameters",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.5 Exploring estimated model parameters",
    "text": "1.5 Exploring estimated model parameters\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables.\nYour trained model, lm_fit, has been loaded into this session.\nPass your trained model object, lm_fit into the appropriate function to explore the estimated model parameters.\nWhich of the following statements is correct?\n\nThe standard error, std.error, for the sqft_living predictor variable is 175.\nThe estimated parameter for the home_age predictor variable is 305.\nThe estimated parameter for the sqft_living predictor variable is 104.\nThe estimated intercept is 127825.\n\n\n\n\n\n\n\nNote\n\n\n\nThe tidy() function automatically creates a tibble of estimated model parameters. Since sqft_living has a positive estimated parameter, the selling price of homes increases with the square footage. Conversely, since home_age has a negative estimated parameter, older homes tend to have lower selling prices."
  },
  {
    "objectID": "01-MWTM.html#predicting-home-selling-prices",
    "href": "01-MWTM.html#predicting-home-selling-prices",
    "title": "1  Machine Learning with tidymodels",
    "section": "Predicting home selling prices",
    "text": "Predicting home selling prices\nAfter fitting a model using the training data, the next step is to use it to make predictions on the test dataset. The test dataset acts as a new source of data for the model and will allow you to evaluate how well it performs.\nBefore you can evaluate model performance, you must add your predictions to the test dataset.\nIn this exercise, you will use your trained model, lm_fit, to predict selling_price in the home_test dataset.\nYour trained model, lm_fit, as well as the test dataset, home_test have been loaded into your session.\n\nInstructions\n\nCreate a tibble, home_predictions, that contains the predicted selling prices of homes in the test dataset.\n\n\n# Predict selling_price\nhome_predictions &lt;- predict(lm_fit,\n                            new_data = home_test)\n\n# View predicted selling prices\nhead(home_predictions) |&gt; \n  kable()\n\n\n\n\n.pred\n\n\n\n\n380968.9\n\n\n473812.2\n\n\n531879.3\n\n\n564689.1\n\n\n450063.9\n\n\n535532.4\n\n\n\n\n\n\nCreate a tibble with the selling_price, home_age, and sqft_living columns from the test dataset and the predicted home selling prices named home_test_results.\n\n\n# Combine test data with predictions\nhome_test_results &lt;- home_test |&gt; \n  select(selling_price, home_age, sqft_living) |&gt; \n  bind_cols(home_predictions)\nhead(home_test_results) |&gt; \n  kable()\n\n\n\n\nselling_price\nhome_age\nsqft_living\n.pred\n\n\n\n\n411000\n18\n1130\n380968.9\n\n\n425000\n11\n1920\n473812.2\n\n\n535000\n3\n2360\n531879.3\n\n\n559900\n20\n2930\n564689.1\n\n\n552321\n29\n1960\n450063.9\n\n\n485000\n6\n2440\n535532.4\n\n\n\n\n# Or\nbroom::augment(lm_fit, new_data = home_test) |&gt; \n  select(.pred, selling_price, home_age, \n         sqft_living) -&gt; home_test_results\nhead(home_test_results) |&gt; \n  kable()\n\n\n\n\n.pred\nselling_price\nhome_age\nsqft_living\n\n\n\n\n380968.9\n411000\n18\n1130\n\n\n473812.2\n425000\n11\n1920\n\n\n531879.3\n535000\n3\n2360\n\n\n564689.1\n559900\n20\n2930\n\n\n450063.9\n552321\n29\n1960\n\n\n535532.4\n485000\n6\n2440\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou have trained a linear regression model and used it to predict the selling prices of homes in the test dataset! The model only used two predictor variables, but the predicted values in the .pred column seem reasonable!"
  },
  {
    "objectID": "01-MWTM.html#evaluating-model-performance-video",
    "href": "01-MWTM.html#evaluating-model-performance-video",
    "title": "1  Machine Learning with tidymodels",
    "section": "Evaluating model performance video",
    "text": "Evaluating model performance video"
  },
  {
    "objectID": "01-MWTM.html#model-performance-metrics",
    "href": "01-MWTM.html#model-performance-metrics",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.6 Model performance metrics",
    "text": "1.6 Model performance metrics\nEvaluating model results is an important step in the modeling process. Model evaluation should be done on the test dataset in order to see how well a model will generalize to new datasets.\nIn the previous exercise, you trained a linear regression model to predict selling_price using home_age and sqft_living as predictor variables. You then created the home_test_results tibble using your trained model on the home_test data.\nIn this exercise, you will calculate the RMSE and \\(R^2\\) metrics using your results in home_test_results.\nThe home_test_results tibble has been loaded into your session.\n\nInstructions\n\nExecute the first two lines of code which print the home_test_results. This tibble contains the actual and predicted home selling prices in the home_test dataset. Using home_test_results, calculate the RMSE and R squared metrics.\n\n\n# Print home_test_results\nhead(home_test_results)\n\n# A tibble: 6 × 4\n    .pred selling_price home_age sqft_living\n    &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 380969.        411000       18        1130\n2 473812.        425000       11        1920\n3 531879.        535000        3        2360\n4 564689.        559900       20        2930\n5 450064.        552321       29        1960\n6 535532.        485000        6        2440\n\n# Calculate the RMSE metric\nhome_test_results |&gt; \n  rmse(truth = selling_price, estimate = .pred) -&gt; ARMSE\nkable(ARMSE)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n49988.89\n\n\n\n\n# Same as\nhome_test_results |&gt; \n  summarize(RMSE = sqrt(mean((selling_price - .pred)^2)))\n\n# A tibble: 1 × 1\n    RMSE\n   &lt;dbl&gt;\n1 49989.\n\n# Calculate the R squared metric\nhome_test_results |&gt; \n  rsq(truth = selling_price, estimate = .pred) -&gt; AR2\nkable(AR2)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6315395\n\n\n\n\n# Same as\nhome_test_results |&gt; \n  summarize(R2 = cor(selling_price, .pred)^2)\n\n# A tibble: 1 × 1\n     R2\n  &lt;dbl&gt;\n1 0.632\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe RMSE metric indicates that the average prediction error for home selling prices is $49,988.89. Not bad considering you only used home_age and sqft_living as predictor variables!"
  },
  {
    "objectID": "01-MWTM.html#r-squared-plot",
    "href": "01-MWTM.html#r-squared-plot",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.7 R squared plot",
    "text": "1.7 R squared plot\nIn the previous exercise, you got an \\(R^2\\) value of 0.6315395. The \\(R^2\\) metric ranges from 0 to 1, 0 being the worst and 1 the best.\nCalculating the \\(R^2\\) value is only the first step in studying your model’s predictions.\nMaking an \\(R^2\\) plot is extremely important because it will uncover potential problems with your model, such as non-linear patterns or regions where your model is either over or under-predicting the outcome variable.\nIn this exercise, you will create an \\(R^2\\) plot of your model’s performance.\nThe home_test_results tibble has been loaded into your session.\n\nInstructions\n\nCreate an \\(R^2\\) plot of your model’s performance. The x-axis should have the actual selling price and the y-axis should have the predicted values. Use the appropriate functions to add the line \\(y = x\\) to your plot and standardize the range of both axes.\n\n\n# Create an R squared plot of model performance\nggplot(home_test_results, aes(x = selling_price, y = .pred)) +\n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = \"dashed\") +\n  coord_obs_pred() +\n  scale_x_continuous(labels = scales::label_currency()) + \n  scale_y_continuous(labels = scales::label_currency()) + \n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price') + \n  theme_bw() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the plot, you can see that your model tends to over-predict selling prices for homes that sold for less than $400,000, and under-predict for homes that sold for $600,000 or more. This indicates that you will have to add more predictors to your model or that linear regression may not be able to model the relationship as well as more advanced modeling techniques!"
  },
  {
    "objectID": "01-MWTM.html#complete-model-fitting-process-with-last_fit",
    "href": "01-MWTM.html#complete-model-fitting-process-with-last_fit",
    "title": "1  Machine Learning with tidymodels",
    "section": "1.8 Complete model fitting process with last_fit()",
    "text": "1.8 Complete model fitting process with last_fit()\nIn this exercise, you will train and evaluate the performance of a linear regression model that predicts selling_price using all the predictors available in the home_sales tibble.\nThis exercise will give you a chance to perform the entire model fitting process with tidymodels, from defining your model object to evaluating its performance on the test data.\nEarlier in the chapter, you created an rsample object called home_split by passing the home_sales tibble into initial_split(). The home_split object contains the instructions for randomly splitting home_sales into training and test sets.\nThe home_sales tibble, and home_split object have been loaded into this session.\n\nInstructions\n\nUse the linear_reg() function to define a linear regression specification. Use the lm engine.\n\n\n# Define a linear regression specification\nlinear_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  set_mode(\"regression\")\n\n\nTrain your linear regression object with the last_fit() function. In your model formula, use selling_price as the outcome variable and all other columns as predictor variables. Create a tibble with the model’s predictions on the test data.\n\n\n# Train linear_model with last_fit()\nlinear_fit &lt;- linear_model |&gt; \n  last_fit(selling_price ~ ., split = home_split)\n\n# Collect predictions and view results\npredictions_df &lt;- linear_fit |&gt; \n  collect_predictions()\npredictions_df |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\n.pred\nid\n.row\nselling_price\n.config\n\n\n\n\n397707.5\ntrain/test split\n3\n411000\nPreprocessor1_Model1\n\n\n440709.5\ntrain/test split\n9\n425000\nPreprocessor1_Model1\n\n\n487982.5\ntrain/test split\n10\n535000\nPreprocessor1_Model1\n\n\n586699.0\ntrain/test split\n14\n559900\nPreprocessor1_Model1\n\n\n469003.7\ntrain/test split\n15\n552321\nPreprocessor1_Model1\n\n\n561843.3\ntrain/test split\n17\n485000\nPreprocessor1_Model1\n\n\n\n\n\n\nCreate an \\(R^2\\) plot of the model’s performance. The x-axis should have the actual selling price and the y-axis should have the predicted values.\n\n\n# Make an R squared plot using predictions_df\nggplot(predictions_df, aes(x = selling_price, y = .pred)) + \n  geom_point(alpha = 0.5) + \n  geom_abline(color = 'blue', linetype = \"dashed\") +\n  coord_obs_pred() +\n  scale_x_continuous(labels = scales::label_currency()) + \n  scale_y_continuous(labels = scales::label_currency()) + \n  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price') + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou have created your first machine learning pipeline and visualized the performance of your model. From the \\(R^2\\) plot, the model still tends to over-predict selling prices for homes that sold for less than $400,000 and under-predict for homes at $600,000 or more, but it is a slight improvement over your previous model with only two predictor variables.\n\n\n\n\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and Hadley Wickham. 2024. Rsample: General Resampling Infrastructure. https://rsample.tidymodels.org.\n\n\nKuhn, Max, and Davis Vaughan. 2025. Parsnip: A Common API to Modeling and Analysis Functions. https://github.com/tidymodels/parsnip.\n\n\nKuhn, Max, and Hadley Wickham. 2024. Tidymodels: Easily Install and Load the Tidymodels Packages. https://tidymodels.tidymodels.org."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Frick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2024. Rsample: General Resampling\nInfrastructure. https://rsample.tidymodels.org.\n\n\nKuhn, Max, and Davis Vaughan. 2025. Parsnip: A Common API to\nModeling and Analysis Functions. https://github.com/tidymodels/parsnip.\n\n\nKuhn, Max, and Hadley Wickham. 2024. Tidymodels: Easily Install and\nLoad the Tidymodels Packages. https://tidymodels.tidymodels.org."
  }
]